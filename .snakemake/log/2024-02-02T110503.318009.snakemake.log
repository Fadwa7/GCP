Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 2
Rules claiming more threads will be scaled down.
Job stats:
job            count
-----------  -------
all                1
fetch_fastq        4
total              5

Select jobs to execute...
Execute 1 jobs...

[Fri Feb  2 11:05:03 2024]
Job 4: fetch fastq from NCBI
Reason: Missing output files: Results/Fastq_Files/SRR7093895.fastq.gz

Terminating processes on user request, this might take some time.
[Fri Feb  2 11:05:07 2024]
Error in rule fetch_fastq:
    jobid: 4
    output: Results/Fastq_Files/SRR7093895.fastq.gz
    log: Results/Supplementary_Data/Logs/SRR7093895.sratoolkit.log (check log file(s) for error details)
    shell:
        
        set +eu &&
        . $(conda info --base)/etc/profile.d/conda.sh &&
        conda activate sratoolkit
	fastq-dump 		--split-spot 		--skip-technical SRR7093895 		--stdout 2>Results/Supplementary_Data/Logs/SRR7093895.sratoolkit.log 	| gzip -c > Results/Fastq_Files/SRR7093895.fastq.gz
	
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Complete log: .snakemake/log/2024-02-02T110503.318009.snakemake.log
WorkflowError:
At least one job did not complete successfully.
