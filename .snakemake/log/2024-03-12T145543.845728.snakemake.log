Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 5
Rules claiming more threads will be scaled down.
Job stats:
job               count
--------------  -------
adapt_trimming        2
all                   1
bcalm                 2
fetch_fastq           2
files_path            1
make_fastqc           2
multi_fastqc          1
reindeer              1
total                12

Select jobs to execute...
Execute 1 jobs...

[Tue Mar 12 14:55:44 2024]
Job 2: fetch fastq from NCBI
Reason: Missing output files: /home/el_fadwa1997/Results/Fastq_Files/ERR12022224.fastq.gz

Terminating processes on user request, this might take some time.
[Tue Mar 12 14:55:46 2024]
Error in rule fetch_fastq:
    jobid: 2
    output: /home/el_fadwa1997/Results/Fastq_Files/ERR12022224.fastq.gz
    log: /home/el_fadwa1997/Results/Supplementary_Data/Logs/SRATOOLKIT/ERR12022224.sratoolkit.log (check log file(s) for error details)
    shell:
        
                        set +eu &&
                        . $(conda info --base)/etc/profile.d/conda.sh &&
                        conda activate sratoolkit
                        prefetch ERR12022224
                        parallel-fastq-dump                         --outdir /home/el_fadwa1997/Results/Fastq_Files                         --gzip                         --sra-id ERR12022224                         --threads 5
			
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Complete log: .snakemake/log/2024-03-12T145543.845728.snakemake.log
WorkflowError:
At least one job did not complete successfully.
